{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b480566-735b-47af-bcd0-e332f5c2957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 MIT Introduction to Deep Learning. All Rights Reserved.\n",
    "#\n",
    "# Â© MIT Introduction to Deep Learning\n",
    "# http://introtodeeplearning.com\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078367b-7fb9-4028-a26b-e8130bcb9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install comet_ml > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe8280-8eb0-4612-9beb-5db52cf6819f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "# TODO: ENTER YOUR API KEY HERE!! instructions above\n",
    "COMET_API_KEY = \"HanneRGFEXJClPevsXNZlFuUa\"\n",
    "\n",
    "# Import PyTorch and other relevant libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Download and import the MIT Introduction to Deep Learning package\n",
    "!pip install mitdeeplearning --quiet\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "# Import all remaining packages\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write\n",
    "#!apt-get install abcmidi timidity > /dev/null 2>&1\n",
    "\n",
    "\n",
    "# Check that we are using a GPU, if not switch runtimes\n",
    "#   using Runtime > Change Runtime Type > GPU\n",
    "assert torch.backends.mps.is_available(), \"Please enable GPU from runtime settings\"\n",
    "assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\"\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Example tensor on MPS\n",
    "x = torch.ones(3, 3).to(device)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dd351-ac88-4223-8703-b21b9c3f14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS Built:\", torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb410f5-53bd-4e04-8949-9b55f5b055a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "songs = mdl.lab1.load_training_data()\n",
    "\n",
    "# Print one of the songs to inspect it in greater detail!\n",
    "example_song = songs[0]\n",
    "print(\"\\nExample song: \")\n",
    "print(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27838aae-748d-4734-b0de-ef19269747a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the ABC notation to audio file and listen to it\n",
    "mdl.lab1.play_song(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30471dc8-a45f-4a1e-bac1-7302d6196c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join our list of song strings into a single string containing all songs\n",
    "songs_joined = \"\\n\\n\".join(songs)\n",
    "\n",
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(songs_joined))\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f032c0b-57da-48d0-aa97-f108faa9ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define numerical representation of text ###\n",
    "\n",
    "# Create a mapping from character to unique index.\n",
    "# For example, to get the index of the character \"d\",\n",
    "#   we can evaluate `char2idx[\"d\"]`.\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "\n",
    "# Create a mapping from indices to characters. This is\n",
    "#   the inverse of char2idx and allows us to convert back\n",
    "#   from unique index to the character in our vocabulary.\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb53353-32ac-4348-9a56-1503ce9ed407",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371f645-71b4-477f-91a4-f3cfff2e143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorize the songs string ###\n",
    "\n",
    "'''TODO: Write a function to convert the all songs string to a vectorized\n",
    "    (i.e., numeric) representation. Use the appropriate mapping\n",
    "    above to convert from vocab characters to the corresponding indices.\n",
    "\n",
    "  NOTE: the output of the `vectorize_string` function\n",
    "  should be a np.array with `N` elements, where `N` is\n",
    "  the number of characters in the input string\n",
    "'''\n",
    "def vectorize_string(string):\n",
    "    N = len(string)\n",
    "    out = np.zeros(N, dtype=np.int8)\n",
    "    for n, ch in enumerate(string):\n",
    "        out[n] = char2idx[ch]\n",
    "    return out\n",
    "\n",
    "\n",
    "vectorized_songs = vectorize_string(songs_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019db086-0c54-4f51-a631-c5db4a96fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
    "# check that vectorized_songs is a numpy array\n",
    "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97d724-aa80-4a61-8483-cca61222650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch definition to create training examples ###\n",
    "\n",
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    # the length of the vectorized songs string\n",
    "    n = vectorized_songs.shape[0] - 1\n",
    "    # randomly choose the starting indices for the examples in the training batch\n",
    "    idx = np.random.choice(n - seq_length, batch_size)\n",
    "\n",
    "    '''TODO: construct a list of input sequences for the training batch'''\n",
    "    input_batch = np.array([vectorized_songs[d:d+seq_length] for d in idx])\n",
    "\n",
    "    '''TODO: construct a list of output sequences for the training batch'''\n",
    "    output_batch = np.array([vectorized_songs[d+1:d+seq_length+1] for d in idx])\n",
    "\n",
    "    # Convert the input and output batches to tensors\n",
    "    x_batch = torch.tensor(input_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(output_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "# Perform some simple tests to make sure your batch function is working properly!\n",
    "test_args = (vectorized_songs, 10, 2)\n",
    "x_batch, y_batch = get_batch(*test_args)\n",
    "assert x_batch.shape == (2, 10), \"x_batch shape is incorrect\"\n",
    "assert y_batch.shape == (2, 10), \"y_batch shape is incorrect\"\n",
    "print(\"Batch function works correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45312d-a0ea-4c89-b1f7-f853371a8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(x_batch[0], y_batch[0])):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx.item()])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx.item()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea251d-afe6-4d22-a031-da4e5551eadb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Defining the RNN Model ###\n",
    "\n",
    "'''TODO: Add LSTM and Linear layers to define the RNN model using nn.Module'''\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define each of the network layers\n",
    "        # Layer 1: Embedding layer to transform indices into dense vectors\n",
    "        #   of a fixed embedding size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
    "\n",
    "        '''TODO: Layer 2: LSTM with hidden_size `hidden_size`. note: number of layers defaults to 1.\n",
    "         Use the nn.LSTM() module from pytorch.'''\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, device=device, batch_first=True) # TODO\n",
    "\n",
    "        '''TODO: Layer 3: Linear (fully-connected) layer that transforms the LSTM output\n",
    "        #   into the vocabulary size.'''\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size, device=device) # TODO\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
    "                torch.zeros(1, batch_size, self.hidden_size).to(device))\n",
    "\n",
    "    def forward(self, x, state=None, return_state=False):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if state is None:\n",
    "            state = self.init_hidden(x.size(0), x.device)\n",
    "        out, state = self.lstm(x, state)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out if not return_state else (out, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc2676-05ae-4dfb-bbfc-a3bb50e9ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model! Build a simple model with default hyperparameters. You\n",
    "#     will get the chance to change these later.\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "# print out a summary of the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9f276-d0bf-4c59-9e84-9353750fa594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with some sample data\n",
    "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "pred = model(x)\n",
    "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2d9d3-76e5-4177-85cc-66ad8d76427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = torch.multinomial(torch.softmax(pred[0], dim=-1), num_samples=1)\n",
    "sampled_indices = sampled_indices.squeeze(-1).cpu().numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95741599-29ba-4983-be25-6b7c80394674",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0].cpu()])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa151d5-7dad-4a34-8409-3563c3e6a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the loss function ###\n",
    "\n",
    "# '''TODO: define the compute_loss function to compute and return the loss between\n",
    "#     the true labels and predictions (logits). '''\n",
    "cross_entropy = nn.CrossEntropyLoss() # instantiates the function\n",
    "def compute_loss(labels, logits):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      labels: (batch_size, sequence_length)\n",
    "      logits: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "    Output:\n",
    "      loss: scalar cross entropy loss over the batch and sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    # Batch the labels so that the shape of the labels should be (B * L,)\n",
    "    batched_labels = labels.view(-1)\n",
    "\n",
    "    ''' TODO: Batch the logits so that the shape of the logits should be (B * L, V) '''\n",
    "    vocab_size = logits.shape[-1]\n",
    "    batched_logits = logits.view(-1, vocab_size) # TODO\n",
    "\n",
    "    '''TODO: Compute the cross-entropy loss using the batched  next characters and predictions'''\n",
    "    loss = cross_entropy(batched_logits, batched_labels) # TODO\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859cd9c-3b07-4c1b-bdc3-32a928fc62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute the loss on the predictions from the untrained model from earlier. ###\n",
    "y.shape  # (batch_size, sequence_length)\n",
    "pred.shape  # (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "'''TODO: compute the loss using the true next characters from the example batch\n",
    "    and the predictions from the untrained model several cells above'''\n",
    "example_batch_loss = compute_loss(y, pred) # TODO\n",
    "\n",
    "print(f\"Prediction shape: {pred.shape} # (batch_size, sequence_length, vocab_size)\")\n",
    "print(f\"scalar_loss:      {example_batch_loss.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3dcbc0-e029-4998-bb72-463f5a61384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a2211-cc90-4894-af9a-3d96010e1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter setting and optimization ###\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Model parameters:\n",
    "params = dict(\n",
    "  num_training_iterations = 2000,  # Increase this to train longer\n",
    "  batch_size = 10,  # Experiment between 1 and 64\n",
    "  seq_length = 100,  # Experiment between 50 and 500\n",
    "  learning_rate = 1e-3,  # Experiment between 1e-5 and 1e-1\n",
    "  embedding_dim = 512,\n",
    "  hidden_size = 1024,  # Experiment between 1 and 2048\n",
    ")\n",
    "\n",
    "# Checkpoint location:\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa2ac9-9760-4dec-b522-609fc87e256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Comet experiment to track our training run ###\n",
    "\n",
    "def create_experiment():\n",
    "  # end any prior experiments\n",
    "  if 'experiment' in locals():\n",
    "    experiment.end()\n",
    "\n",
    "  # initiate the comet experiment for tracking\n",
    "  experiment = comet_ml.Experiment(\n",
    "                  api_key=COMET_API_KEY,\n",
    "                  project_name=\"SM_Lab1_Part2\")\n",
    "  # log our hyperparameters, defined above, to the experiment\n",
    "  for param, value in params.items():\n",
    "    experiment.log_parameter(param, value)\n",
    "  experiment.flush()\n",
    "\n",
    "  return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22777b57-e97e-436d-a806-ea1ee2f55d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define optimizer and training operation ###\n",
    "\n",
    "'''TODO: instantiate a new LSTMModel model for training using the hyperparameters\n",
    "    created above.'''\n",
    "model = LSTMModel(vocab_size, params['embedding_dim'], params['hidden_size'])\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "'''TODO: instantiate an optimizer with its learning rate.\n",
    "  Checkout the PyTorch website for a list of supported optimizers.\n",
    "  https://pytorch.org/docs/stable/optim.html\n",
    "  Try using the Adam optimizer to start.'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "def train_step(x, y):\n",
    "  # Set the model's mode to train\n",
    "  model.train()\n",
    "\n",
    "  # Zero gradients for every step\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Forward pass\n",
    "  '''TODO: feed the current input into the model and generate predictions'''\n",
    "  y_hat = model(x)\n",
    "\n",
    "  # Compute the loss\n",
    "  '''TODO: compute the loss!'''\n",
    "  loss = compute_loss(y, y_hat)\n",
    "\n",
    "  # Backward pass\n",
    "  '''TODO: complete the gradient computation and update step.\n",
    "    Remember that in PyTorch there are two steps to the training loop:\n",
    "    1. Backpropagate the loss\n",
    "    2. Update the model parameters using the optimizer\n",
    "  '''\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "    \n",
    "  return loss\n",
    "\n",
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "experiment = create_experiment()\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "for iter in tqdm(range(params[\"num_training_iterations\"])):\n",
    "\n",
    "    # Grab a batch and propagate it through the network\n",
    "    x_batch, y_batch = get_batch(vectorized_songs, params[\"seq_length\"], params[\"batch_size\"])\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    x_batch = torch.tensor(x_batch, dtype=torch.long).to(device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    # Take a train step\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # Log the loss to the Comet interface\n",
    "    experiment.log_metric(\"loss\", loss.item(), step=iter)\n",
    "\n",
    "    # Update the progress bar and visualize within notebook\n",
    "    history.append(loss.item())\n",
    "    plotter.plot(history)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if iter % 100 == 0:\n",
    "        torch.save(model.state_dict(), checkpoint_prefix)\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), checkpoint_prefix)\n",
    "experiment.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9b00b-d91e-4c7e-9bd2-245a956ab13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction of a generated song ###\n",
    "\n",
    "def generate_text(model, start_string, generation_length=1000):\n",
    "  # Evaluation step (generating ABC text using the learned RNN model)\n",
    "\n",
    "  '''TODO: convert the start string to numbers (vectorize)'''\n",
    "  input_idx = [char2idx[s] for s in start_string] # TODO\n",
    "  # input_idx = ['''TODO''']\n",
    "  input_idx = torch.tensor([input_idx], dtype=torch.long).to(device)\n",
    "\n",
    "  # Initialize the hidden state\n",
    "  state = model.init_hidden(input_idx.size(0), device)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  tqdm._instances.clear()\n",
    "\n",
    "  for i in tqdm(range(generation_length)):\n",
    "    '''TODO: evaluate the inputs and generate the next character predictions'''\n",
    "    predictions, state = model(input_idx, state, return_state=True)\n",
    "    # predictions, hidden_state = model('''TODO''', '''TODO''', return_state=True)\n",
    "\n",
    "    # Remove the batch dimension\n",
    "    predictions = predictions.squeeze(0)\n",
    "\n",
    "    '''TODO: use a multinomial distribution to sample over the probabilities'''\n",
    "    input_idx = torch.multinomial(torch.softmax(predictions, dim=-1), num_samples=1)\n",
    "    # input_idx = torch.multinomial('''TODO''', dim=-1), num_samples=1)\n",
    "\n",
    "    '''TODO: add the predicted character to the generated text!'''\n",
    "    # Hint: consider what format the prediction is in vs. the output\n",
    "    text_generated.append(idx2char[input_idx].item()) # TODO\n",
    "    # text_generated.append('''TODO''')\n",
    "\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eae91b-32f2-4fc2-9d52-fd8bcef674e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: Use the model and the function defined above to generate ABC format text of length 1000!\n",
    "    As you may notice, ABC files start with \"X\" - this may be a good start string.'''\n",
    "generated_text = generate_text(model, start_string=\"H\", generation_length=1000) # TODO\n",
    "# generated_text = generate_text('''TODO''', '''TODO''', '''TODO''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c0e30-dadf-4277-8d46-07a4d5aefb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Play back generated songs ###\n",
    "\n",
    "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
    "\n",
    "for i, song in enumerate(generated_songs):\n",
    "  # Synthesize the waveform from a song\n",
    "  waveform = mdl.lab1.play_song(song)\n",
    "\n",
    "  # If its a valid song (correct syntax), lets play it!\n",
    "  if waveform:\n",
    "    print(\"Generated song\", i)\n",
    "    ipythondisplay.display(waveform)\n",
    "\n",
    "    numeric_data = np.frombuffer(waveform.data, dtype=np.int16)\n",
    "    wav_file_path = f\"output_{i}.wav\"\n",
    "    write(wav_file_path, 88200, numeric_data)\n",
    "\n",
    "    # save your song to the Comet interface -- you can access it there\n",
    "    experiment.log_asset(wav_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a28e9-7935-4e57-ac01-43224ae70a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when done, end the comet experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d52a9-bf65-4b33-8f10-5e5cc3ed2d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
